# 2주차
**발표일**: 2025.02.07
### Team 1
- **Members**: 전민하, 조은비
- 참고 논문:  
[Attention is All You Need](https://arxiv.org/pdf/1706.03762)

---
### Team 2
- **Members**: 강한결, 조수빈
- 참고 논문:  
[ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/pdf/2003.10555)

---
### 질의응답  
- 조은비🐱

  🩷 받은 질문 : Results에 있는 table의 설명은 논문에 자세히 있는 건지, 아니면 따로 찾아본건지  
  
     -> 답변 : 설명이 있기는 하지만 상세하지는 않다. 발표 준비를 하면서 직접 많이 찾아보고 자료를 만들었다.  

  🩷 받은 질문 : Big 모델과 base 모델의 차이가 유의미 하지 않다고 저자들이 말하고 있는지  
  
     -> 답변 : '유의미하지 않다'고 하지는 않았다. 논문에서 Big 모델이 최고 성능을 기록했지만, 그럼에도 불구하고 Base 모델이 충분히 우수하고 학습 비용이 훨씬 적게 든다
                는 점을 비교해주는 부분이 있다.

  🩷 받은 질문 : base 모델을 먼저 두고, 나머지 A, B 이런식으로 모델들을 비교한건가  
  
     -> 답변 : 그렇다. 구성요소의 중요성을 평가하기 위해 base 모델을 다양한 방법으로 변형해 봤다고 설명하고 있다.


- 전민하🐼
 
  🩵 받은 질문: warmup의 효과 관련, 학습률 기울기가 증가하는데 학습이 어떻게 안정해지는지?  
  
     -> 답변: warmup을 사용하여 학습률을 서서히 증가시키면, 초반의 불안정한 학습을 방지할 수 있음. 즉, 학습률을 __제어__ 할 수 있다는 것에 의미를 두면 됨.

---
### 소감
- 조은비 : Transformer 원리를 조금 더 이해할 수 있게 돼 뿌듯하다. 만약 스터디를 하지 않았다면 아직도 이해도가 굉장히 낮은 상태였을 것 같다. 발표를 하고 질문을 받으면서 스스로 생각해보지 못했던, 질문들을 받으면서 더 생각해보는 계기가 됐다. Team 2가 준비해온 Bert 변형모델 발표에서는 RoBERTa, ALBERT, ELECTRA 등 다양한 모델을 설명들을 수 있어서 좋았다. hidden vector라는 생소한 개념들도 신기하고 재밌었다.

 
- 전민하: 논문 하나를 읽고 정리했다는 점에 감명 받았고, transformer 의 기초가 되는 내용이라 더 의미가 깊음. 귀찮고 힘들지만 스터디를 했음에 또 하나의 보람과 지식을 가지게 되었기에 감정은 사라지고 결과만 남는다는 구절을 다시금 마음에 새김. 읽지 않은 Bert 논문에 대해서도 다른 팀의 발표를 들으며 모델들의 차이점을 조금이나마 알게 됨. 다음 번엔 어떤 논문을 리뷰할지 고민하는 과정까지 이어가며 남은 시간까지 열심히 해보고 싶음. 다들 화이팅 ϵ( ‘Θ’ )϶


---
### 질의응답  
- 최수빈🦝

  🩶 받은 질문: All-Tokens MLM, LECTRA  
  
     -> 답변: All-Tokens MLM: 마스크된 토큰뿐만 아니라 모든 입력 토큰의 정체성을 예측하는 방식. BERT와 ELECTRA 사이의 성능 차이를 대부분 해소했으며, ELECTRA의 핵심 이점이 "모든 입력 토큰으로부터 학습"하는 데서 온다는 것을 보여줌
  
  ELECTRA가 All-Tokens MLM보다도 더 나은 성능을 보인다는 점에서, ELECTRA의 이점은 단순히 더 많은 학습 데이터에서 오는 것이 아님을 시사한다.
   

  🩶 받은 질문: 두 문장의 차이  
  
     -> 답변: all-tokens MLM은 모든 토큰을 마스킹 해서 추론하는 모델이고 ELECTRA는 대체로 생성해서 원시 코퍼스와 비교하며 이게 진짜인지 가짜인지 구별하는 모델이라서 이 부분에서 차이가 있다.

  MLM은 전체 데이터에서 15%만 마스킹하기 때문에 데이터의 크기가 중요했었는데 ELECTRA는 전체를 학습하기 때문에 작은 데이터로도 학습이 가능하며(이 부분이 RTD의 핵심 아이디어다.)

  즉 저 말들은 일부만 가리는 것(MLM)과 전체를 가려보고 맞추는 것(RTD) 중 어느 부분이 더 효율적인가를 논하는 말들로, 전체를 학습하는 게 더 결과가 좋기 때문에 데이터의 크기가 크게 상관 없다는 뜻으로 해석된다.

  ![image](https://github.com/user-attachments/assets/e642692c-4513-4e9a-87e7-86421e360ecb)

- 강한결🐻  
  
  🤎 받은 질문 : hidden vector에 Embedding/Positional/Segment Embedding값이 모두 더해져서 들어있는데도 모델이 각각의 의미를 이해하고 학습할 수 있는 이유는?
  
     -> 답변 : word embedding은 -0.1~0.1, positional embedding은 sin, cos값, segment embedding은 정수값(1,2,3...)을 가지므로 다 더하더라도 embedding vector들이 각각의 영역을 갖게 되고 학습이 진행되면서 모델이 그것을 인식한다.
  
---
### 소감
- 최수빈 : 지난 번에 비해서 이번엔 많이 준비했었는데, 말도 많이 더듬고 이해했다고 생각했는데 몇가지는 까먹기도 해서 발표가 많이 아쉬웠다. 다만 이 아쉬운 건 내가 그만큼 많이 준비했기 때문이리라. 발표를 하지 않았다면 이렇게 치열하게 논문을 탐독해보지도 않았을 것이고 스터디를 하지 않았다면 논문을 읽어볼 생각조차 하지도 않았을 것이다. 아이러니 한 것이 준비할 때는 괴로웠고 많이 힘들었는데, 막상 끝나니 더 잘하지 못해서 아쉽다는 생각이 드는 걸 보면 논문을 읽고 이해하고 발표하는 이 과정들에 재미가 들린게 분명하다.
시간이 지난다면 여기서 보고 공부하고 들은 기억들이 많이 희석되고 까먹겠지만 그래도 이게 이런거지라고 얘기할 수 있는 수준으로 남았으면 좋겠다. 또 까먹었다면 우리가 정리한 문서들을 보고 다시 이해하면 될 것이 아닌가? 이번 발표는 저번보다 더 재밌었고, 여러모로 의미깊었던 발표회인 것 같다.

- 강한결 : 발표를 열심히 준비했어도 프로젝트 진행하면서 며칠 안 봤더니 이해했다고 생각했던 부분들도 다시 헷갈렸던 것은 좀 아쉽다. 그렇지만 2회 동안 양 팀이 학습한 것을 토대로 Transformer와 BERT, 그 변형모델들의 재료가 되는 다양한 아이디어들을 톺아보며 ml, dl 모델들이 어떻게 만들어지고 어떻게 활용될 것인지에 대한 인사이트를 많이 얻은 것 같다. 바쁜 와중에도 스터디가 흐지부지되지 않고 잘 진행되어서 보람차다.
