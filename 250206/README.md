# 2주차
**발표일**: 2025.02.07
## Team 1
- **Members**: 전민하, 조은비
### 참고 논문
1. [Attention is All You Need](https://arxiv.org/abs/1706.03762)

---
## Team 2
- **Members**: 강한결, 조수빈
### 참고 논문

---
### 질의응답
- 조은비
받은 질문 : Results에 있는 table의 설명은 논문에 자세히 있는 건지, 아니면 따로 찾아본건지
-> 답변 : 설명이 있기는 하지만 상세하지는 않다. 발표 준비를 하면서 직접 많이 찾아보고 자료를 만들었다.

받은 질문 : Big 모델과 base 모델의 차이가 유의미 하지 않다고 저자들이 말하고 있는지
-> 답변 : '유의미하지 않다'고 하지는 않았다. 논문에서 Big 모델이 최고 성능을 기록했지만, 그럼에도 불구하고 Base 모델이 충분히 우수하고 학습 비용이 훨씬 적게 든다는 점을 비교해주는 부분이 있다.

받은 질문 : base 모델을 먼저 두고, 나머지 A, B 이런식으로 모델을을 비교한건가
-> 답변 : 그렇다. 구성요소의 중요성을 평가하기 위해 base 모델을 다양한 방법으로 변형해 봤다고 설명하고 있다.

---
### 소감
- 조은비 : Transformer 원리를 조금 더 이해할 수 있게 돼 뿌듯하다. 만약 스터디를 하지 않았다면 아직도 이해도가 굉장히 낮은 상태였을 것 같다. 발표를 하고 질문을 받으면서 스스로 생각해보지 못했던, 질문들을 받으면서 더 생각해보는 계기가 됐다. Team 2가 준비해온 Bert 변형모델 발표에서는 RoBERTa, ALBERT, ELECTRA 등 다양한 모델을 설명들을 수 있어서 좋았다. hidden vector라는 생소한 개념들도 신기하고 재밌었다.



---
### 소감
- 최수빈 : 지난 번에 비해서 이번엔 많이 준비했었는데, 말도 많이 더듬고 이해했다고 생각했는데 몇가지는 까먹기도 해서 발표가 많이 아쉬웠다. 다만 이 아쉬운 건 내가 그만큼 많이 준비했기 때문이리라. 발표를 하지 않았다면 이렇게 치열하게 논문을 탐독해보지도 않았을 것이고 스터디를 하지 않았다면 논문을 읽어볼 생각조차 하지도 않았을 것이다. 아이러니 한 것이 준비할 때는 괴로웠고 많이 힘들었는데, 막상 끝나니 더 잘하지 못해서 아쉽다는 생각이 드는 걸 보면 논문을 읽고 이해하고 발표하는 이 과정들에 재미가 들린게 분명하다.
시간이 지난다면 여기서 보고 공부하고 들은 기억들이 많이 희석되고 까먹겠지만 그래도 이게 이런거지라고 얘기할 수 있는 수준으로 남았으면 좋겠다. 또 까먹었다면 우리가 정리한 문서들을 보고 다시 이해하면 될 것이 아닌가? 이번 발표는 저번보다 더 재밌었고, 여러모로 의미깊었던 발표회인 것 같다.

추가로 은비님께서 질문주셨던, 

---
All-Tokens MLM: 마스크된 토큰뿐만 아니라 모든 입력 토큰의 정체성을 예측하는 방식. BERT와 ELECTRA 사이의 성능 차이를 대부분 해소했으며, ELECTRA의 핵심 이점이 "모든 입력 토큰으로부터 학습"하는 데서 온다는 것을 보여줌

LECTRA가 All-Tokens MLM보다도 더 나은 성능을 보인다는 점에서, ELECTRA의 이점은 단순히 더 많은 학습 데이터에서 오는 것이 아님을 시사한다.

---

두 문장의 차이를 질의주셨는데, gpt한테 물어보니 그 대답을 축약하면 다음과 같다


all-tokens MLM은 모든 토큰을 마스킹 해서 추론하는 모델이고 ELECTRA는 대체로 생성해서 원시 코퍼스와 비교하며 이게 진짜인지 가짜인지 구별하는 모델이라서 이 부분에서 차이가 있다.
MLM은 전체 데이터에서 15%만 마스킹하기 때문에 데이터의 크기가 중요했었는데 ELECTRA는 전체를 학습하기 때문에 작은 데이터로도 학습이 가능하며(이 부분이 RTD의 핵심 아이디어다.)
즉 저 말들은 일부만 가리는 것(MLM)과 전체를 가려보고 맞추는 것(RTD) 중 어느 부분이 더 효율적인가를 논하는 말들이다.

![image](https://github.com/user-attachments/assets/e642692c-4513-4e9a-87e7-86421e360ecb)

---
