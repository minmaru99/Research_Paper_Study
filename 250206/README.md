# 2주차
**발표일**: 2025.02.07
## Team 1
- **Members**: 전민하, 조은비
### 참고 논문
1. [Attention is All You Need](https://arxiv.org/abs/1706.03762)

---
## Team 2
- **Members**: 강한결, 조수빈
### 참고 논문

---
### 질의응답
- 조은비
받은 질문 : Results에 있는 table의 설명은 논문에 자세히 있는 건지, 아니면 따로 찾아본건지
-> 답변 : 설명이 있기는 하지만 상세하지는 않다. 발표 준비를 하면서 직접 많이 찾아보고 자료를 만들었다.

받은 질문 : Big 모델과 base 모델의 차이가 유의미 하지 않다고 저자들이 말하고 있는지
-> 답변 : '유의미하지 않다'고 하지는 않았다. 논문에서 Big 모델이 최고 성능을 기록했지만, 그럼에도 불구하고 Base 모델이 충분히 우수하고 학습 비용이 훨씬 적게 든다는 점을 비교해주는 부분이 있다.

받은 질문 : base 모델을 먼저 두고, 나머지 A, B 이런식으로 모델을을 비교한건가
-> 답변 : 그렇다. 구성요소의 중요성을 평가하기 위해 base 모델을 다양한 방법으로 변형해 봤다고 설명하고 있다.

---
### 소감
- 조은비 : Transformer 원리를 조금 더 이해할 수 있게 돼 뿌듯하다. 만약 스터디를 하지 않았다면 아직도 이해도가 굉장히 낮은 상태였을 것 같다. 발표를 하고 질문을 받으면서 스스로 생각해보지 못했던, 질문들을 받으면서 더 생각해보는 계기가 됐다. Team 2가 준비해온 Bert 변형모델 발표에서는 RoBERTa, ALBERT, ELECTRA 등 다양한 모델을 설명들을 수 있어서 좋았다. hidden vector라는 생소한 개념들도 신기하고 재밌었다.