
# 👽 6주차

**발표일**: 2025.07.04

---

# ⚙️ 주제
## 생성형 AI(GPT-3)
- 발표자: **전민하, 최수빈**  
- 참고 논문:  
[Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)

## 역전파  
- 발표자: **강한결, 조은비**  
- 참고 논문:  
[Learning representations by back-propagating errors](http://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf)

---

# 🗣️ 회고
## 전민하  
### 받은 질문  
  :

### 소감  
   : 
   
----

## 최수빈
### 받은 질문  
   :

### 소감  
   : 
   
---
## 강한결
### 받은 질문
Q1. sigmoid 값이 0.8 이상이거나 0.2 미만인 경우만 정/오답으로 처리한다고 했는데, 그 사이의 값들이 나오면 어떻게 처리하는지?
A1. 실제로 모델을 설계한다면 다른 모델을 함께 활용하거나(앙상블), 오분류로 처리한 후 그 데이터를 바탕으로 미세조정하는 등의 방법이 있겠지만, 논문에서 나온 실험의 경우 이미 104개 케이스에 대해 모두 정/오답을 맞혔기 때문에 따로 처리할 일은 없었을 것.

Q2. 왜 임계치가 하필 0.2와 0.8일까?
A2. 논문에 명시적으로 설명되어 있는 부분은 없음. 그냥 일반적으로 적당한 값 넣어서 쓸 듯? 만약에 모든 정답 뉴런의 sigmoid 값이 0.9 이상이었으면 사후적으로 기준을 그렇게 바꾸지 않았을까 싶기도 하고..
  
### 소감
  : 부트캠프에서 역전파에 대해 처음 배울 때는 세부 알고리즘이나 수식을 뜯어 보지 않고 넘어가서 '저건 평생 이해하지 못하겠구나.'라는 식으로 생각했는데, 막상 논문을 읽어 보니 크게 어렵지 않고 chain rule과 편미분 기본 공식만 알고 있어도 충분히 이해할 수 있는 내용이어서 놀랍고 재밌었다. 빠르게 배우려다보면 많은 학습내용을 비유를 통해 어렴풋이 이해하고 넘어가게 되는데 논문을 통해 사실과 최대한 닮은 형태로 머릿속에 넣어두는 것이 도움이 되는 때가 있었으면 좋겠다.
   
---

## 조은비

### 받은 질문
Q1. 가중치 감쇠를 통해 의미 없는 특성들을 없애는 것이, 모델의 리소스 대비 효율성만 올려주는 건지, 전체적인 성능 자체를 끌어올려주는 건지 궁금합니다.

### 소감  
   : Fig5의 아이디어를 낸 게 가장 신기했다. 역전파 알고리즘을 일방향 구조에 넣은 것도 신기한데, 여기서 더 나아가서 양방향 순환 네트워크에도 역전파를 적용시키기 위해서 시간 순을 넣어서 펼쳤다는 것 자체가, 어떻게 이런 아이디어를 처음으로 생각해낼 수 있었을까 싶어서 그 연구자들의 열의에 감탄했다
