# Deep Learning 관련 논문 리딩 스터디


## 📌 소개
Deep Learning 관련 논문을 읽고 발표하며, 질의응답을 통해 깊이 있는 이해를 목표로 하는 스터디입니다.


## 👥 Team Members
- 강한결
- 전민하
- 조은비
- 최수빈


## 📜 Rules
1. **질문 및 답변 정리**
   - 질문자는 **질문 내용**과 **답변 내용**을 정리합니다.
   - 각자 **소감**을 작성한 뒤, 끝난 후 이를 취합하여 하나의 파일로 정리합니다.

2. **파일 관리**
   - GitHub 리포지토리에서 아래의 파일을 체계적으로 관리합니다:
     - 논문 원본 및 번역본
     - 발표 자료
     - 질의응답 내용
     - 각자의 소감

3. **질문 진행 방식**
   - 질문은 발표가 끝난 후에 진행합니다.
   - 답변이 잘 되지 않은 질문은 **다음 주 발표 때까지 준비**하여 다시 답변합니다.
 
---

## 📆 주차별 진행 내용

| 주차  | 논문 제목 및 내용 |
|-------|------------------------------|
| 1주차 | Attention Is All You Need  |
|       | What does bert learn about the structure of language    |
|       | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding   |
| 2주차 | Attention Is All You Need  |
|       | BERT 변형 모델              |
|       | ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators              |
| 3주차 | YOLO 모델별 비교   |
|       | A Brief Survey of Deep Reinforcement Learning |
| 4주차 | You Only Look Once: Unified, Real-Time Object Detection |
|       | DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning |
|       | LLaMA: Open and Efficient Foundation Language Models |
|       | Llama 2: Open Foundation and Fine-Tuned Chat Models |


> 발표 자료 및 논문 정리는 각 폴더를 통해 확인 가능합니다.
