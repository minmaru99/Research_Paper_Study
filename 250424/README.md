# 👽 5주차

**발표일**: 2025.04.24

---

# ⚙️ 주제
## 생성형 AI(GPT-3)
- 발표자: **전민하, 최수빈**  
- 참고 논문:  
[Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)

## 역전파  
- 발표자: **강한결, 조은비**  
- 참고 논문:  
[Learning representations by back-propagating errors](http://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf)

---

# 🗣️ 회고
## 전민하  
### 받은 질문  
   - Q1: 왜 모든 토큰에 가중치를 동일하게 주는가?
   - A1: GPT-3의 학습 방식인 self-supervised prediction(자기지도학습) 때문임. 사전학습 시 문장의 일부분을 숨기고 빈칸을 맞추는 방식으로 배우는데, 어떤 부분이 가려질지는 무작위로 정해지기 때문에 모든 단어가 동등한 확률로 예측 대상이 되는 것! 모델은 어떤 단어가 더 중요한지 모름.

### 소감  
   : 매일같이 사용하는 gpt의 발전 방향과 한계를 명확하게 알게 되어 좋았다. 특히 지난 발표 때 딥러닝 모델의 탄소발자국 이야기를 듣고 전력 소비에 대해 관심이 생겼는데, 대규모 언어 모델의 에너지 문제를 알게 되는 기회가 되었으며 더 나아가 성별/인종/종교 관련 사회적 영향 부분이 흥미로웠다. 사람들이 남긴 텍스트를 대량으로 학습하는 모델이기에 사회 전반적으로 내포되어 있는 편견 또한 모델이 똑같이 가지고 있음이 괘씸하고 안타까웠다. 역전파 발표를 듣던 중 신경망의 구조가 1950~60년대에 고안됐다는 점이 믿기지 않았고 뒷내용이 궁금했다.
   
----

## 최수빈
### 받은 질문  
   - Q1: 우리가 과정에서 했던 프롬프트와 발표문에 있던 inference가 같은 것이라 했는데 어떻게 같은 것인지?
   - A1: inference는 어떠한 개념이 아니고, 그저 이해를 돕기 위해 비유문처럼 사용된 구절이다. 아이디어인 in-context learning이 주어진 내용을 그대로 받아들이고 이해한다는 점에서, 뜻이 추론인 inference란 단어를 사용한 것. 정답의 형식을 지정해주는 게 프롬프팅이니깐, 입력받은 내용을 추론(Inference)하는 과정이 발생하는데 이게 in-context learning이다.

*추가*

논문에서는 해당 단어와 in-context learning과 별도의 개념으로 설명하거나 정의하지 않으며, 모델이 fine-tuning없이 문제를 푸는 시점을 Inference time이라고 표현하는 것을 확인

### 소감  
   : 우리에겐 너무 익숙한 gpt가 어떤 아이디어를 통해서 발전하는지 알 수 있어서 좋았다. 또 나는 뒷부분의 내용을 미처 다 파악하지 못하였는데, 민하님의 발표를 들은 순간 gpt가 보통 사람들의 편파적인 시선을 학습하다는 사실에 왠지 입맛이 씁쓸했었다. 사람들은 인공지능이 터미네이터같은 영화처럼 사람들에게 악영향을 끼칠 거라고 두려워하지만, 그 아이디어를 제공하고 학습시키는 것은 사람이란 걸 떠올린다면 굉장히 모순적인 부분이라고 느껴진다. gpt와 같은 인공지능의 발전을 통해 우리는 더 윤택하고 훌륭한 삶을 바라지만, 실상은 우리 인류의 부정적인 부분을 답습한다는 점에서, 결국엔 우리가 우리 스스로를 망치는데 일조하는 게 아닐까? 하는 암울한 생각마저 들 정도였다. 사람들은 ai를 규제해야 한다고 외치지만, 정작 그 방향과 방법론을 제시하지 못한다는 점이 떠올랐는데, 이 논문을 공부하고 나니 정작 반성하고 자정해야 하는 것은 우리 스스로가 아닐까?
   퍼셉트론이란 것에 대해서 다시 알 수 있어서 좋았고, 우리 과정에서 배웠던 내용들을 리프레쉬 하는 기분으로 들어서 산뜻했던 것 같다. 한결님이 아쉽게 출석하지 못하셔서 뒷부분 내용을 듣지 못한 게 아쉬웠다. 그 부분이 재밌는 내용이 많다고 강조하셨기에, 빨리 듣고싶다는 생각 뿐이다.
   또 개인적으로 이번에는 다른 떼에 비해서 말도 좀 덜 절고, 준비한 내용도 이전 회차들보다는 조금 더 낫게 발표한 것 같다는 생각이 든다. 내용을 파악하려고 더욱 애쓰는 태도와 발표 내용 이해에 대한 욕구가 더 커지는 만큼 전체적인 발표 태도가 많이 향상되고 있다고 스스로 느낀다. 하지만 여전히 매끄러운 설명에 대해선 아쉽다는 생각이 드는데, 내 딴에는 잘 이해하리라 하고 부연설명한 것이 오히려 오해를 불러온 것이 아쉽게 느껴졌다.
   그리고 이번에 청년센터란 걸 처음 이용해 봤는데, 신축 건물이라 그런지 쾌적하고, 시설도 되게 훌륭했다. 장소대여 가격이 만만찮은 시점에서, 적당한 공간을 발견하지 못한다면 다른 구의 센터를 이용해보는 것도 좋은 계획이 될 것 같다. 
   
---
## 강한결
개인 사정으로 불참ㅠㅠ
   
---

## 조은비
### 받은 질문  
   - Q1: Fig1 실험에서 중요하게 봐야 하는 부분이 '대칭을 이뤘다'인건가? 그리고 가중치끼리 더하면 1이 나오지는 않는 거 같다.
   - A1: Fig1 실험에서 중요하게 봐야 하는 부분은 '대칭이면 1, 아니면 0'이라는 조건을 줬을 때, 은닉 유닛의 내부표현학습으로 '대칭성'이라는 개념을 알아내고, 대칭인지 아닌지를 구분할 수 있는 가중치 비율과 값, bias를 스스로 부여했다는 점이다. 그리고 가중치끼리 더하는 게 아니라 수식1에서 net input을 구하고, 그걸 수식2처럼 sigmoid를 통해 출력값을 얻는다. 0에 가까워지면 '꺼졌다'고 표현하고, 1에 가까워지면 '켜졌다'고 표현한다.
   - Q2: XOR에 대해 반말과 존댓말에 대한 예시를 보여주셨는데, 영문에서는 적용되기 어려울 거 같다. 다른 예시가 있나.
   - A2: 과일의 당도와 산도라는 특징이 있을 때, 0, 1 또는 1, 0이면 선호과일(1)이고, 0, 0이거나 1, 1이면 비선호과일(0)이다. 

### 소감  
   : 역전파 논문을 조금 더 이해하게 돼 만족스럽다. 은닉 유닛이 스스로 내부표현학습을 하게 한 최초의 실험을 공부할 때는 논문 저자들에게 존경심까지 생겼다. 다른 팀의 발표에서는 in context learnig이 어떤 단점을 보완하려고 생겨났는지, 앞으로의 개선점과 목표지향점은 무엇인지를 알게돼 좋았다. 그리고 self-supervisedprediction에서 어떤 단어가 문제로 나올지 모르니 토큰 가중치를 모두 똑같이 줬다는 부분도 흥미로웠다.  
