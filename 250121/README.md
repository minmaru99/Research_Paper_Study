# 1주차
**발표일**: 2025.01.21
### Team 1
- **Members**: 전민하, 조은비
- 참고 논문:  
  [Attention is All You Need](https://arxiv.org/pdf/1706.03762)

---
### Team 2
- **Members**: 강한결, 조수빈
- 참고 논문:  
  [BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)  
  [What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/pdf/1906.04341)

---

### 강한결

#### Transformer에 대한 질의응답
- **Scaled dot-product attention이 어디에 있는 거지?**  
  Encoder와 Decoder 모두에 있는 Multi-head-attention 내부에 있다.  
  병렬적으로 h번 수행되면서 내적연산을 통해 각 단어의 중요도를 계산한다.

- **multi-head attention의 h값은 하이퍼파라미터인가?**  
  맞다. 모델의 성능에 큰 영향을 끼치는 하이퍼파라미터이다.  
  논문에서 다룬 original 모델에서는 8을 사용했고, 태스크가 복잡해질수록 더 높은 h값을 사용해야 한다. 

- **attention에서 -무한대값을 사용하는 이유가 뭐지?**  
  패딩 토큰과 미래 단어는 학습에서 무시되어야 하는 요소인데,  
  해당 단어의 출력함수인 softmax함수를 계산할 때 분자에 들어가는 변수에 -무한대를 대입함으로써  
  결과값을 0으로 수렴시킨다. 이를 통해 특정 요소를 학습에 포함되지 않도록 설정할 수 있다.

- **같은 레이어 내에서는 같은 가중치와 bias값을 쓴다고 했는데, 성질이 비슷한 레이어끼리도 다른 층이면 다른 값을 쓰나?**  
  성질이 비슷하더라도 다른 층끼리는 독립적인 가중치와 bias를 사용한다.  
  데이터는 레이어를 지나면서 단계적으로 처리되고, 따라서 모델의 모든 레이어는 역할이 서로 조금씩 다르다.

#### 소감
발표 자료를 준비하면서 공부한 것도 큰 도움이 되었지만, 실제 발표와 질문·답변 시간을 통해 배운 확장된 내용이 훨씬 더 알차고 가치 있었다. 기대했던 것보다 훨씬 재미있었고, 이러한 경험이 앞으로도 계속 이어졌으면 좋겠다는 생각이 들었다. AI의 역사에 대해 지속적으로 공부해 나갈 텐데, 지금은 이해가 부족한 영역도 있지만, 매주 충실히 학습해 나가다 보면 나중에 이 논문을 다시 돌아봤을 때 훨씬 폭넓게 받아들일 수 있을 것 같아 기대가 된다.

---

### 전민하

#### BERT에 대한 질의응답
- **논문에서 쓰인 MASK가 transformer의 mask와 동일한지?**  
  같은 mask인 건 맞지만 변형이 있을 것으로 예상됨.

- **문장에서 추출되지 않은 85%와, 추출된 15% 중 원본이 유지되는 10%의 차이가 무엇인지?**  
  후자(10%)는 결국 예측에 반영이 된다는 것이 차이.

- **모델의 전체적인 성능이 떨어지는 이유는 무엇인지?**  
  무작위 추출의 영향(연결성 문제), 글 전체 길이의 영향.

- **하층의 학습에서는 '최소단위'인 단어로 나누어 학습하는데, 영어와 달리 한국어는 형태소로 나뉜다는 점에서 학습에 차이가 있는지?**  
  있을 것. KoBert를 사용해보길!

- **문장 내 띄어쓰기 등 고려할 요소가 있음에도 중층 학습에서 구 단위로 묶는 다는 게 신기함.**  
  영어는 한국어보다 띄어쓰기의 영향이 크지 않음.

- **WLNI가 무엇인지?**  
  대명사가 가리키는 대상.

#### 소감
transformer의 근본이 되는 논문을 읽는 경험 자체가 유익하다고 생각했는데, 발표를 준비하고 설명하며 질문에 답하는 과정까지 어느 하나 도움되지 않은 부분이 없었습니다. 너무 이해가 안 돼서 대충 설명하고 포기하고 싶었지만 시간을 들여 이것저것 찾아보고 습득하려고 한 제가 기특해요.. transformer 팀과는 다른 방식으로 Bert의 핵심 및 추가 논문을 통해 내용을 정리한 Bert팀의 발표는 논문을 간편하게 이해할 수 있어서 정말 좋았습니다.

---

### 조은비

#### BERT에 대한 질의응답
- 내가 한 질문 : **Transfomer에서 더 발전된 걸로 유명한 Bert에서 mask 말고 또 어떤 부분이 발전되고 활용됐는지 궁금합니다 🙂**  
  들은 답변 : 기존 모델을 기반으로 만든 모델들도 기존 메커니즘을 조금씩 바꿔가며 쓰는 것 같던데 정확히 어떤 부분인지는 모르겠다. 다음주 발표 때 준비해서 말씀드리겠다.

#### 소감
오르지 못할 나무처럼 까마득하게 느껴졌던 Transfomer 논문.  
그래도 계속 보다 보니 처음에 이해가 안 가던 부분이 이해가 되기도 하고, 조금씩 어떤 원리인지 이해가 갈 때마다 이 논문을 발표한 분들에 대한 존경심이 생기고, 얼마나 대단한 논문인지 새삼 느껴지기도 했습니다.  
팀 발표에서 제가 이해한 부분을 잘 설명하고 싶었는데 잘 못한 거 같아서 허탈했지만 팀원분들이 생각보다 이해에 도움이 됐고, 도움이 됐다고 해주셔서 다행이었습니다.  
아직도 완전히 이해 못한 부분들이 남아있는데 조금씩 공부해서 완전히 이해하는 날이 왔으면 좋겠습니다.

---

### 최수빈

#### Transformer에 대한 질의응답
1. **내적곱을 왜 사용하는가?**  
   디멘션의 크기를 동일하게 가져가기 위함.

2. **경로가 짧다는 것의 의미?**  
   공간적 제약이 없기 때문에 시간을 소요한다는 개념이 없는 상태이다.

3. **해당 질문에 대한 GPT 문답**  
   네가 이해한 대로, 초반의 정보가 문장이 끝날 때까지 지속적으로 뒤 단어들에게 영향을 미치고 연결됩니다.  
   이 과정은 Self-Attention의 병렬성과 정보 전달 효율성 덕분에 가능하며, 문장 전체의 문맥을 잘 반영할 수 있는 강력한 구조를 만들어줍니다.

4. **위와 같은 방식으로 계산 진행 시, 계속해서 단어의 메모리가 눈덩이처럼 불어나는데 효율적인 방식인지?**  
   Self-Attention에서는 모든 단어 간의 관계를 한 번에 병렬 계산할 수 있어.  
   예: "나는", "수학을", "공부하는" 등이 동시에 연산됨.  
   GPU/TPU를 활용하면 병렬 계산으로 효율성이 대폭 향상돼.

#### 소감
살면서 논문이란 걸 이렇게 자세히 탐독해본 적이 있었을까?  
논문스터디가 아니었다면 이 진귀한 경험을 하기까지의 시간이 더 오래 걸렸을 것이다.  
간단한 발표를 위해 모르겠어도 계속해서 읽어보고 모르는 부분을 GPT와 주고받으며 조금이라도 이해해보려고 노력했던 이 경험은 매우 성공적이었고 쾌감이 가득했다.  
내가 이해했던 것들을 온전히 드러내지 못했다는 아쉬움도 들면서도, 또 다른 분들의 준비를 보며 내가 너무 만만하게 생각했다는 깨달음에, 발표를 보는 것은 즐거우면서도 참회의 시간이 되었던 것 같았다.  
이전에 'attention is all you need'를 읽었을 때는 몰랐던 내용도 알게 되었고, 그때 대강 알았던 것들과 오늘의 발표를 통해서 다시 한번 되세김을 할 수 있어서 매우 유익했다.  
또한 이번에 'bert' 논문을 준비하면서 모델의 처리 방법에 대해서 만큼은 어떻게 작동하는지 설명할 수 있는 단계에 이르렀다고 판단이 되어서 매우 뿌듯했다.  
이 모든 것들은 함께 목표를 정하고 결과물을 공유하기로 정했기에 일어날 수 있는 일들이었고, 오늘의 발표는 두 말할 것 없이 매우 유익했다고 단언할 수 있다.  
다음엔 비슷한 주제 다른 내용으로 진행하게 될 텐데 그때는 더욱 전심으로 달려들어보자.

