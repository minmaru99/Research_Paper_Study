# 1주차

**발표일**: 2025.01.21

## Team 1
- **Members**: 전민하, 조은비

### 참고 논문
1. [Attention is All You Need](https://arxiv.org/abs/1706.03762)

---

## Team 2
- **Members**: 강한결, 조수빈

### 참고 논문
1. [BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
2. [What Does BERT Learn About the Structure of Language?](https://arxiv.org/abs/1906.04341)

## 논문 스터디 발표 정리

### 강한결
#### 질의응답
1. **Scaled dot-product attention이 어디에 있는 거지?**
   - Encoder와 Decoder 모두에 있는 Multi-head-attention 내부에 있다.
   - 병렬적으로 h번 수행되며, 내적 연산을 통해 각 단어의 중요도를 계산한다.

2. **Multi-head attention의 h값은 하이퍼파라미터인가?**
   - 맞다. 성능에 큰 영향을 끼치는 하이퍼파라미터이다.
   - 논문의 original 모델에서는 8을 사용했으며, 태스크가 복잡할수록 높은 h값이 필요하다.

3. **Attention에서 -무한대값을 사용하는 이유는?**
   - 패딩 토큰과 미래 단어는 학습에서 무시되어야 한다.
   - Softmax 계산 시, 분자에 -무한대를 대입해 결과값을 0으로 수렴시킴으로써 특정 요소를 학습에서 제외한다.

4. **같은 레이어 내에서는 같은 가중치와 bias값을 쓰지만, 다른 층끼리는 독립적인가?**
   - 성질이 비슷하더라도 다른 층끼리는 독립적인 가중치와 bias를 사용한다.

#### 소감
- 발표 준비와 질문·답변 과정에서 확장된 내용을 배웠다.
- AI 역사와 관련된 지속적인 공부를 통해 논문을 폭넓게 이해할 수 있을 것 같다.

---

### 전민하
#### 질의응답
1. **논문에서 쓰인 MASK가 transformer의 mask와 동일한가?**
   - 동일하지만 변형이 있을 것으로 예상된다.

2. **문장에서 추출되지 않은 85%와 추출된 15% 중 원본이 유지되는 10%의 차이는?**
   - 후자(10%)는 예측에 반영된다는 점이 차이이다.

3. **모델의 전체적인 성능 저하 이유는?**
   - 무작위 추출, 글 전체 길이 등의 영향을 받는다.

4. **한국어와 영어의 학습 차이는?**
   - 영어는 단어를 최소 단위로, 한국어는 형태소를 최소 단위로 학습하여 차이가 있을 것이다.

5. **WLNI란?**
   - 대명사가 가리키는 대상.

#### 소감
- 발표 준비부터 질문 답변까지 유익한 경험이었다.
- 처음에는 이해가 어려웠지만, 점차 내용을 습득하는 과정에서 성취감을 느꼈다.

---

### 조은비
#### 질의응답
1. **BERT에서 mask 외에 발전된 부분은?**
   - 기존 메커니즘에서 일부 변형이 이루어졌으며, 자세한 내용은 추가 학습 필요.

#### 소감
- 처음에는 까마득하게 느껴졌던 논문이 점차 이해되기 시작했다.
- 팀 발표와 질문 답변 과정을 통해 내용을 정리하고 공유한 것이 유익했다.

---

### 최수빈
#### 질의응답
1. **내적곱을 사용하는 이유는?**
   - 디멘션 크기를 동일하게 유지하기 위함.

2. **경로가 짧다는 의미는?**
   - 공간적 제약 없이 정보를 병렬적으로 처리해 효율성이 높다는 의미.

3. **Self-Attention에서의 효율성?**
   - 병렬 처리로 모든 단어 간의 관계를 동시에 계산함.

#### 소감
- 논문 탐독과 발표 준비 과정에서 얻은 경험이 유익했다.
- 발표 과정에서 부족한 점을 느끼며 학습의 필요성을 깨달았다.
- Transformer와 BERT의 작동 원리를 설명할 수 있게 되어 성취감을 느꼈다.

---
